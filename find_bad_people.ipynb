{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping bad guys \n",
    "## Sabrina Fuller (sf8ez@virginia.edu)\n",
    "\n",
    "#### 1. first and formost I decided to to do this in a python notebook as it is more interactive and easier to test small bits of code. Espcially since working with regex , I needed to be testing as I go. \n",
    "\n",
    "#### 2. I decided to use beautiful soup, because I have used beautiful soup to scrape wikipedia before (the data formatting was much easier). \n",
    "### ________________________________________________________________________\n",
    "\n",
    "#### The webscraper_a class is goes to the requested url and scrapes all the pages that that page links to, then linearly converts the html to json\n",
    "\n",
    "#### the formatting_data class formats my data such that it can read into a json\n",
    "\n",
    "### ________________________________________________________________________\n",
    "#### current issues : \n",
    "###### My regular expressions are not working right \n",
    "###### I blieve the issue is due to the regex search expressions not working right\n",
    "###### Since I haven't really worked intensly with regex I need more time to read the docs to understand the timing issue \n",
    "###### Formatting scraps details my current regex searches (they work for some) but not all data on the site\n",
    "###### http://www.regular-expressions.info/catastrophic.htmlCurrent \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import dateutil.parser as dparser\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import dateutil.parser as dparser\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "\n",
    "## imports the beautiful soup object\n",
    "#only had time to really work on formatting the regular expressions\n",
    "URL = 'https://eumostwanted.eu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class webscraper_a: \n",
    "    ## turns a python dict into a json file\n",
    "    def make_json(self):\n",
    "        js = None\n",
    "        with open('tst.json', 'w') as json_file:\n",
    "            json.dump(self.page_data_dict, json_file)\n",
    "            js = self.json_table\n",
    "    \n",
    "        return \n",
    "           \n",
    "    ##Get page takes an url as input and returns the html parsed page\n",
    "    ## uses the beautuful soup package\n",
    "    def get_page(self, url ):\n",
    "        try: \n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            self.curr_page = soup\n",
    "            return soup\n",
    "        except: \n",
    "            print(\"exception thrown\")\n",
    "            return None \n",
    "\n",
    "     #Sets the main head page (the root of the crawler tree)   \n",
    "    def set_main_soup(self): \n",
    "        self.main_soup = self.get_page(self.url)\n",
    "        return \n",
    "        \n",
    "    #get all the urls from this page url    \n",
    "    def get_page_urls(self):\n",
    "        #returns the urls\n",
    "        try: \n",
    "            page_results = self.main_soup.find(\"div\", class_ = \"view-content\").findAll(\"a\", recursive = \"false\")\n",
    "            for i in page_results: \n",
    "                self.page_urls.append(i[\"href\"])\n",
    "        except AttributeError: \n",
    "            print(\"error in reading the page urls\")\n",
    "        return \n",
    "    #get content for the subpages\n",
    "    def get_page_content(self, url):\n",
    "        self.curr_page = self.get_page(url)\n",
    "        print(\"getting page content for: \", url)\n",
    "        #returns the urls\n",
    "        try: \n",
    "            page_results = self.curr_page.find(\"div\", class_ = \"content\")\n",
    "            return page_results\n",
    "        except AttributeError: \n",
    "            print(\"error in reading the page\")\n",
    "            return None \n",
    "        \n",
    "    \n",
    "    ## constructor \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        #current page crawler is on (for debugging)\n",
    "        self.curr_page = None\n",
    "        #root page\n",
    "        self.main_soup = None\n",
    "        #child page of root\n",
    "        self.side_soup = None \n",
    "        #list of page urls to visit\n",
    "        self.page_urls = []\n",
    "        #dict to eventually become the json\n",
    "        self.page_data_dict = []\n",
    "        #json \n",
    "        self.json_table = None\n",
    "        #get the main page\n",
    "        self.get_page(self.url)\n",
    "        return\n",
    "    #The main webscrape method, once the webscraper objected is constructed\n",
    "    # this method is able to run without any additonal input\n",
    "    ## possible upgrades\n",
    "        ## Set tree crawl depth and navigate pages to that depth\n",
    "        ## function to name_json \n",
    "        ## create error catching functions \n",
    "    def webscrape(self): \n",
    "        #make the main soup\n",
    "        \n",
    "        soup = self.set_main_soup()\n",
    "        #get the urls from the page\n",
    "        self.get_page_urls()\n",
    "        formatter_ = data_formatting(self.url, self.curr_page.content)\n",
    "        #progress\n",
    "        print(\"------------------------------\")\n",
    "        print(len(self.page_urls),\"subpages found \")\n",
    "        #loop through each url and parse the data\n",
    "        data__list = []\n",
    "        for i in range(len(self.page_urls)): \n",
    "            self.side_soup = self.get_page_content(self.page_urls[i])\n",
    "            formatter_ = data_formatting(self.page_urls[i], self.side_soup)\n",
    "            page_data = formatter_.parse_html()\n",
    "            #self.page_data_dict.append(page_data) \n",
    "            data__list.append(page_data)\n",
    "            \n",
    "            \n",
    "        #make the json   \n",
    "        data__list\n",
    "        pd.Dataframe(data__list)\n",
    "        #json_file = self.make_json()    \n",
    "        return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_formatting: \n",
    "    ## Updates should include better exception catching\n",
    "    ## Better parsing of data to allow for more generality\n",
    "    ## A way to define certain expression patterns the user could enter \n",
    "    ## constructor for data_formatting\n",
    "    def __init__(self, url, html): \n",
    "        self.url = url\n",
    "        self.data = None\n",
    "        self.site_html = html\n",
    "        # predefined rows of data\n",
    "        # I struggled on to use the data on the page to name my rows\n",
    "        # but not all people had the same datafields so I needed this baseline\n",
    "        # I know I'm missing some data fields\n",
    "        self.data_rows = [\"name\", \"date of birth\",  \"sex\", \"height\",  \"eye color\",\"ethic orgin\", \n",
    "                      \"languages spoken\", \"nationality\", \n",
    "                      \"crime\", \"case status\", \"source url\"]\n",
    "        return \n",
    "    ## Data patterns, essentially a dictionary of various patterns to look for\n",
    "    def data_patterns_regex(self,data_type, url):\n",
    "        self.url = url\n",
    "        #matches the datatype ith the correct pattern\n",
    "        pattern_type = {\n",
    "            #date pattern\n",
    "            1: (\"(Jan(uary)?|Feb(ruary)?|Mar(ch)?|Apr(il)?|May|Jun(e)?|\"\"Jul(y)?|Aug(ust)?|Sep(tember)?|Oct(ober)?|Nov(ember)?|\"\"Dec(ember)?)\\s+\\d{1,2},\\s+\\d{4}\"), \n",
    "            #https://stackoverflow.com/questions/54058389/regex-match-month-name-day-year\n",
    "            #crime pattern\n",
    "            8: r'Crime:(.*) (?=Sex:)|(?= Nationality:)| (?= Ethnic Origin:)|(?= State of Case:)',\n",
    "            #case state             \n",
    "            9: \"State of case((.|\\n)+)(.+)((.|\\n)+)(.*)(?=Reward)\",\n",
    "            4: \"Eye colour:(.+?) \",\n",
    "            5: \"Ethic Origin:(.+?) \",\n",
    "           #language pattern \n",
    "            7: \"Language spoken:(.+?) \",\n",
    "            6:\"Nationality:(.+?) \",\n",
    "            #search for string after \n",
    "            2:\"Sex: (.+?) \", \n",
    "            3: \"Approximate height:(.+?) \",\n",
    "            #pattern for name\n",
    "            0: \"([A-Z\\s])(.*) (?=Wanted)|(?=Crime)\",\n",
    "            10: \"(.*) other(.*) \"\n",
    "            }\n",
    "\n",
    "        return pattern_type.get(data_type, \"na\")\n",
    "    ## the main html parser\n",
    "    def parse_html(self):\n",
    "       \n",
    "            cleanr= re.compile('<.*?>')#remove the html\n",
    "            cleantext = re.sub(cleanr, '', str(self.site_html)) #get the clean htm;\n",
    "            #cleanr = unicodedata.normalize('NFKD', cleantext).encode('ascii', 'ignore')\n",
    "            \n",
    "            data_obj = []\n",
    "\n",
    "           \n",
    "            for i in self.data_rows:\n",
    "                #i, self.url, \"current scrape\")\n",
    "                if(i == \"date of birth\"):\n",
    "                    d = self.data_patterns_regex(1, self.url)\n",
    "                    date_patt = re.compile(d)\n",
    "                    try:\n",
    "                        dob = date_patt.search(cleantext).group() \n",
    "                        data_obj.append(dob)  \n",
    "                    except: \n",
    "                        dob = \"none\"\n",
    "                elif (i == \"case status\"): \n",
    "                    case_state_patt = self.data_patterns_regex(9, self.url)\n",
    "                    data_obj.append(re.findall(case_state_patt, cleantext))   \n",
    "                    \n",
    "                else: \n",
    "                    patt = self.data_patterns_regex(self.data_rows.index(i), self.url)\n",
    "                    data_obj.append(re.search(patt, cleantext))\n",
    "                \n",
    "                    \n",
    "\n",
    "            \n",
    "            self.data = data_obj\n",
    "        \n",
    "            self.fill_rows()\n",
    "            return  self.data\n",
    "\n",
    "    ## this populates my dict that I will eventually turn into a json\n",
    "    def fill_rows(self):\n",
    "        data_list = []\n",
    "        \n",
    "        print(\"filling rows!\")\n",
    "        for k in self.data:\n",
    "            try: \n",
    "                \n",
    "                s = k[0].strip() \n",
    "                data_list.append(s)   \n",
    "            except:\n",
    "                data_list.append(\"none\")\n",
    "                \n",
    "                \n",
    "        try:       \n",
    "            data_list.append(str(self.url))\n",
    "            self.data_rows.append(\"URL\")\n",
    "            #res = dict(zip( d, self.data))\n",
    "            res = dict(zip(data_list, self.data))\n",
    "        \n",
    "            \n",
    "        except: \n",
    "            print(\"error in formatting data\")\n",
    "            res = dict()\n",
    "        print(res)\n",
    "        return res\n",
    "\n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webscraper_a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-aef6668f9d99>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-aef6668f9d99>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebscraper_a\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwebscrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'webscraper_a' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    a = webscraper_a(URL)\n",
    "    a.webscrape()\n",
    "\n",
    "main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Current issue http://www.regular-expressions.info/catastrophic.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
